{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiaqiaoZhang/XCS231N/blob/main/RNNs_and_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTMs in PyTorch\n",
        "In this section, we'll investigate how to implement an LSTM in PyTorch that uses image embeddings from ResNet."
      ],
      "metadata": {
        "id": "NSEWSQmlhAtb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_143-lzvg8_h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision as tv\n",
        "from torchvision.datasets import MovingMNIST\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspecting the Data\n"
      ],
      "metadata": {
        "id": "DcmOdHK2AQAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MovingMNIST(root=\".\", split=\"train\", download=True)"
      ],
      "metadata": {
        "id": "znRbRWPgikZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The dataset has {len(train_dataset)} items\")\n",
        "\n",
        "for item in train_dataset:\n",
        "  print(\"Each data point has shape\", item.shape)\n",
        "  fig, axes = plt.subplots(1, 10, figsize=(20, 2))\n",
        "  for i in range(item.shape[0]):\n",
        "      img = item[i, 0, :, :]\n",
        "      axes[i].imshow(img, cmap='gray')\n",
        "      axes[i].axis('off')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  break"
      ],
      "metadata": {
        "id": "5Su_uqW0irqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "for batch_data in dataloader:\n",
        "  # batch_data will have shape (B,S,C,H,W)\n",
        "  input = torch.repeat_interleave(batch_data[:,:9], repeats=3, dim=2).permute(1,0,2,3,4)\n",
        "  target = batch_data[:,1:].permute(1,0,2,3,4)\n",
        "  print(\"Input Shape:\", input.shape)\n",
        "  print(\"Output Shape:\", target.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "LUZiROnzjFD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In other words, we are feeding the model 8 sequences, each containing 9 64x64 images.\n",
        "\n",
        "Next, we will use a pre-trained CNN and use it to create embeddings for our image"
      ],
      "metadata": {
        "id": "fyMXb2FzltNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building an Encoder and Decoder"
      ],
      "metadata": {
        "id": "8BbYPMZKAWjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding Backbone\n",
        "resnet_net = tv.models.resnet18(weights=\"DEFAULT\")\n",
        "modules = list(resnet_net.children())[:-1]\n",
        "backbone = torch.nn.Sequential(*modules)\n",
        "\n",
        "test_inp = torch.rand(8,3,64,64)\n",
        "backbone(test_inp).shape"
      ],
      "metadata": {
        "id": "CE5SVn2B72cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder Module\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer5(x)\n",
        "        return x\n",
        "\n",
        "decoder = Decoder()\n",
        "input_tensor = torch.randn(8, 512, 1, 1)\n",
        "output_tensor = decoder(input_tensor)\n",
        "print(output_tensor.shape)  # Should output torch.Size([1, 1, 64, 64])"
      ],
      "metadata": {
        "id": "GexEcneLAjXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building an LSTM Model"
      ],
      "metadata": {
        "id": "CmP5VC35DyCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.nn.LSTM(input_size=512, hidden_size=512) # Expects input of shape (Seq Length, Batch Size, Embedding Dim)"
      ],
      "metadata": {
        "id": "uKWDcRoFlRfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_lstm_inp = torch.rand(9,8,512)\n",
        "out, (hidden_state, cell_state) = model(test_lstm_inp)\n",
        "print(\"LSTM Output Shape:\", out.shape)"
      ],
      "metadata": {
        "id": "7d83OJZ8ENBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting Everything Together"
      ],
      "metadata": {
        "id": "XzVeYBvDE9Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch_data in enumerate(tqdm(dataloader)):\n",
        "  batch_size = batch_data.shape[0]\n",
        "  # Make Input RGB, and of shape (L,B,C,H,W)\n",
        "  input = torch.repeat_interleave(batch_data[:,:9], repeats=3, dim=2).permute(1,0,2,3,4)\n",
        "  # No need to make Output RGB\n",
        "  target = batch_data[:,1:].permute(1,0,2,3,4)\n",
        "\n",
        "  # Don't pass gradients to the ResNet, we don't want to change those weights\n",
        "  with torch.no_grad():\n",
        "    L, B, C, H, W = input.shape\n",
        "    backbone_inp = input.reshape(-1,C,H,W).float()\n",
        "    embedded_input = backbone(backbone_inp).reshape(L, B, -1)\n",
        "\n",
        "  model_preds, (h, c) = model(embedded_input)\n",
        "\n",
        "  decoder_inp = model_preds.reshape(L*B, -1, 1, 1)\n",
        "  decoded_image = decoder(decoder_inp).reshape(L, B, 1, 64, 64)\n",
        "\n",
        "  if i == 0:\n",
        "    print(\"Preds shape\", decoded_image.shape)\n",
        "    print(\"GT Shape\", target.shape)"
      ],
      "metadata": {
        "id": "IFBCkXzi8Ku-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize outputs\n",
        "# Select the 0th element along dim=1 for both tensors\n",
        "decoded_img_0 = decoded_image[:, 0, :, :].detach().numpy()\n",
        "target_0 = target[:, 0, :, :].detach().numpy()\n",
        "\n",
        "# Visualize the two sequences of images in a grid\n",
        "fig, axes = plt.subplots(nrows=2, ncols=9, figsize=(18, 4))\n",
        "\n",
        "for i in range(9):\n",
        "    # Display decoded_img_0 images in the first row\n",
        "    axes[0, i].imshow(decoded_img_0[i].squeeze(), cmap='gray')\n",
        "    axes[0, i].axis('off')\n",
        "    axes[0, i].set_title(f'Decoded Img {i+1}')\n",
        "\n",
        "    # Display target_0 images in the second row\n",
        "    axes[1, i].imshow(target_0[i].squeeze(), cmap='gray')\n",
        "    axes[1, i].axis('off')\n",
        "    axes[1, i].set_title(f'Target Img {i+1}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tJYNC2p1KmD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now use your favorite loss function and optimizer! Anything like the L2 loss works well too"
      ],
      "metadata": {
        "id": "6g6zj9vxJyHe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Transformer in PyTorch"
      ],
      "metadata": {
        "id": "4ewh7iDpLLZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vit = tv.models.vit_b_32(weights=\"DEFAULT\")"
      ],
      "metadata": {
        "id": "nR4ss8daLPSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_inp = torch.rand(16, 3, 224, 224)\n",
        "out = vit(random_inp)\n",
        "out.shape"
      ],
      "metadata": {
        "id": "ahIt8FT2LWAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input sizes and Vision Transformer\n",
        "In a Vision Transformer, an image is divided into fixed-size non-overlapping patches, and each patch is linearly embedded into a flat vector. These vectors are then processed by the transformer layers. Since the transformer layers expect a fixed sequence length, the number of patches in the image must remain constant. If the input image size changes, the number of patches will also change, leading to a mismatch in the transformer's input sequence length.\n",
        "\n",
        "In contrast, Convolutional Neural Networks (CNNs) use convolutional layers that apply filters with a sliding window mechanism. These layers can process images of varying sizes because the sliding window operation is inherently local and translation-invariant. As long as the spatial dimensions (height and width) are large enough to accommodate the filter size, the convolution can be applied. However, the output size will change depending on the input size, and any fully connected layers later in the network must be adapted accordingly."
      ],
      "metadata": {
        "id": "LzrUmLb1P5Ed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's investigate the output dimensionality"
      ],
      "metadata": {
        "id": "qJDqtM9rL7d6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary"
      ],
      "metadata": {
        "id": "KTazZ_AtM6tE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit"
      ],
      "metadata": {
        "id": "4JqnaFx7Orl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## You can now use a suitable decoder to develop models for classification, regression, segmentation, and so on."
      ],
      "metadata": {
        "id": "PdhXtKnoQtp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz"
      ],
      "metadata": {
        "id": "kjrMyVfnMV2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviz import make_dot\n",
        "from IPython.display import Image, display\n",
        "\n",
        "dot = make_dot(out, params=dict(list(vit.named_parameters())))\n",
        "# Save the dot file as a PNG\n",
        "dot.format = 'png'\n",
        "dot.render('rnn_torchviz')\n",
        "\n",
        "# Display the saved PNG in the notebook\n",
        "display(Image('rnn_torchviz.png'))"
      ],
      "metadata": {
        "id": "sfcNdoC6L-xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aWPLl2IGOuOf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}